\documentclass{llncs}
\newcommand{\Section}[1]{\vspace{-8pt}\section{\hskip-1em.~~#1}\vspace{-3pt}}
\newcommand{\SubSection}[1]{\vspace{-3pt}\subsection{\hskip -1em.~~#1}\vspace{-3pt}}
\newcommand{\X}{{\bf X}}
\newcommand{\x}{{\bf x}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\y}{{\bf y}}
\newcommand{\Z}{{\bf Z}}
\newcommand{\z}{{\bf z}}
\newcommand{\bs}{\boldsymbol}
\usepackage{amsmath,amssymb,algorithm,algorithmic}
\usepackage{times}
\usepackage{setspace,verbatim}
\usepackage{epsfig,url}

\begin{document}
\vspace{-0.1in}
\title{Partial Sparse Canonical Correlation Analysis (PSCCA) for population
  studies in Neuroimaging}
\author{Anonymous}
\institute{Anonymous}
\maketitle              
%\vspace{-0.1in}
\begin{abstract}
Sparse canonical correlation analysis (SCCA) is a powerful,
multivariate statistical tool for making unbiased inferences about the
relationship between different types of measurements taken on the same
population. Generally, SCCA depends upon univariate models to
factor out the effects of confounding variables.  In this paper we present a
novel algorithm for computing partial sparse canonical correlation
analysis (PSCCA) and factor (``partial'') out the effect of unwanted covariates. PSCCA employs
three views (modalities) of the data with one of them treated as a special set of
measurements whose effect on either  one or both other modalities should be factored out.  For instance, the relationship between
cognition and neuroanatomy should be measured independently of the
effect of age and gender.  We provide  theory and algorithms underlying partial sparse
canonical correlation analysis (PSCCA) via a representative neuroimaging
application.  
\end{abstract}
%\vspace{-0.2in}
\section{Introduction}
% pubmed references to MRI :  
% 2000-2001 --- 25561
% 2001-2002 --- 27053
% 2003-2004 --- 31708
% 2005-2006 --- 38620
% 2008-2009 --- 49288
% 2009-2010 --- 49323
% pubmed references to MRI brain :  
% 2000-2001 --- 9938
% 2001-2002 --- 
% 2003-2004 --- 12655
% 2005-2006 --- 
% 2008-2009 --- 
% 2009-2010 --- 19676
The number of neuroimaging studies published annually doubled from
9938 in 2000-2001 to 19676 in 2009-2010
(\url{http://www.ncbi.nlm.nih.gov/pubmed/}).  This growth has been
accompanied by increasing diversity in the types of data being
collected such that neuroimaging studies include not only various
structural and functional modalities but also neurocognitive
batteries, genetics and environmental measurements.  At the same
time, the statistical methods used in neuroimaging have changed
relatively little from twenty years ago with the exception of a small
number of recent studies \cite{Tosun2010a}.

%Why SCCA
%Why PSCCA

Consider a standard neuroimaging population study.  Each subject in
the study is between 20 and 40 years of age, has a quantitative
imaging measurement, is a member of one of two groups (patient or
control) and is either male or female.  A standard regression analysis
will treat the quantitative imaging measurement as the dependent
variable ($\y$) and age, gender and diagnosis group as covariates $({\X})$ where
each test is performed independently at each voxel.  Detection power
is compromised by the multiple-comparison problem incurred by the
number of imaging measurements (millions) as well as the confounding
variables (age, gender).  If we are using linear regression then we can not do much about the first problem but for the second problem we can factor out the effects of these unwanted (confounding) variables by regressing on the residuals ($\y -\X\beta$). 


An alternative methodology that one can use to approach the above neuroimaging  problem is by using CCA (Canonical Correlation Analysis)~\cite{hotellingcca} which inherently handles the multiple-comparison problem arising from large number of imaging measurements as it maximizes the correlation between the two views of the data ($\Y$ and $\X$ in our case)~\footnote{Note that here {$\X$} is same as earlier but $\Y$ is a matrix of all the imaging measurements and not a vector as was the case for linear regression study. As far as notation is concerned, we denote matrices by uppercase bold fonts and vectors by lowercase bold fonts, throughout this paper. }. Recently, sparse versions of CCA~\cite{parkhomenko,witten,lykou} have been proposed which give sparse loadings and work in the high dimensional setting, $p >n$, where $n$ is the number of observations (number of subjects in our neuroimaging study) and $p$ is the number of covariates. 

We are unaware of any previous work which studies factoring (``partialing'') out the confounding covariates in the case of Sparse CCA. In this paper, we provide the theory of Partial Sparse CCA (PSCCA) and present a novel and efficient iterative algorithm for PSCCA. As mentioned earlier, PSCCA like CCA does not do multiple-comparisons but rather performs a single global sparse multivariate test  over the whole region of interest (e.g. all voxels in the cortex). The sparseness parameter in
PSCCA selects the subset of the brain most associated with the
variable of interest (diagnosis) while factoring out confounding
effects (age, gender).  It is up to the researcher to use his or her
domain knowledge to select the desired sparseness for a given problem
or to experiment with a training dataset to optimize the sparseness
parameter.

The rest of the paper is organized as follows, in the next Section we provide a brief review of CCA and Sparse CCA. In Section 3, we present the theory behind Partial Sparse CCA (PSCCA) and describe our novel iterative algorithm for performing PSCCA. In Section 4, we provide experimental results on synthetic and real world neuroimaging datasets and we conclude with a brief summary in Section 5.

%What we are doing here ( a few sentences )

%This paper will detail and illustrate our approach to performing
%neuroimaging studies in the style of traditional formulations but
%using a new, powerful multivariate pscca.  We highlight in both
%simulated and real data the advantages and disadvantages of this
%method.

%\vspace{-0.2in}

\section{Brief Review: CCA and Sparse CCA (SCCA)}

CCA~\cite{hotellingcca} is the analog to Principal
Component Analysis (PCA) for pairs of matrices. PCA
computes the directions of maximum covariance between elements in a
single matrix, whereas CCA computes the directions of maximal correlation
between a pair of matrices. 
See~\cite{taylor:cca} for a general review of CCA along with some representative applications. 
Unlike PCA, CCA does not depend on how the observations
are scaled.  

More specifically, given a set of $n$ paired observation vectors
$\{(y_1,x_1),...,(y_n,x_n)\}$--in our case the two matrices are the
quantitative imaging measurement ({\Y}) and age, gender, diagnosis ({\X}) matrices\footnote{For
  notational convenience we assume that samples have zero mean.} --we would like to simultaneously find the directions
${\bs{\bs\phi_{\Y}}}$ and
${\bs{\bs\phi_{\X}}}$ that maximize the correlation of
the projections of ${\Y}$ onto ${\bs{\bs\phi_{\Y}}}$
with the projections of ${\X}$ onto
${\bs{\bs\phi_{\X}}}$. This is expressed as

\begin{equation}
\label{cca1}
\rho=\max_{{\bs\phi_Y}, {\bs\phi_X}}
\frac{{\bs\phi_{\X}^T\bs\Sigma_{\X\Y}}\bs\phi_{\Y}}{\sqrt{\bs\phi_{\X}^T\bs\Sigma_{\X\X}\bs\phi_{\X}}\sqrt{\bs\phi_{\Y}^T\bs\Sigma_{\Y\Y}\bs\phi_{\Y}}}
\end{equation}
where ${\bs\Sigma_{\X\X}}$, ${\bs\Sigma_{\Y\Y}}$ and ${\bs\Sigma_{\X\Y}}$ are the auto and cross covariance matrices i.e. $\X^T\X$, $\Y^T\Y$ and $\X^T\Y$ respectively. The above objective can also be thought of as maximizing the numerator $\bs\phi_{\X}^T\bs\Sigma_{\X\Y}\bs\phi_{\Y}$ subject to $\bs\phi_{\X}^T\bs\Sigma_{\X\X}\bs\phi_{\X} =1$ and $\bs\phi_{\Y}^T\bs\Sigma_{\Y\Y}\bs\phi_{\Y}=1$

Now, define change of basis as:

\begin{equation}
\label{basisChange}
\bs\psi_{\X} = \bs\Sigma_{\X\X}^{1/2}\bs\phi_{\X}, \;\;\;\;\;\;   \bs\psi_{\Y} = \bs\Sigma_{\Y\Y}^{1/2}\bs\phi_{\Y} 
\end{equation}

Now, substituting~(\ref{basisChange}) in~(\ref{cca1}) we get 

\begin{equation}
\label{subs}
\rho= \max_{{\bs\psi_{\Y}}, {\bs\psi_{\X}}} \frac{\bs\psi_{\X}^T \bs\Sigma_{\X\X}^{-1/2}\bs\Sigma_{\X\Y}\bs\Sigma_{\Y\Y}^{-1/2}\bs\psi_Y}{\|\bs\psi_{\X}\| \|\bs\psi_{\Y}\|}
\end{equation}

Applying the whitening transform~\footnote{Whitening transform is used to convert covariances to correlations and also to de-correlate the auto-correlation matrices so that CCA can ``truly'' maximize the cross-correlation. The standard whitening transform is defined as $\X_w= \X\bs\Sigma_{\X\X}^{-1/2}$ and $\Y_w= \Y\bs\Sigma_{\Y\Y}^{-1/2}$} to~(\ref{subs})

\begin{equation}
\label{simplifiedcca}
Corr(\X_w\bs\psi_{\X},\Y_w\bs\psi_{\Y})=\rho=\max_{{\bs\psi_{\Y}}, {\bs\psi_{\X}}} \frac{\bs\psi_{\X}^T \bs\Sigma_{\X_w\Y_w}\bs\psi_Y}{\|\bs\psi_{\X}\| \|\bs\psi_{\Y}\|}
\end{equation}
where $\bs\Sigma_{\X_w\Y_w} = \X_w^T\Y_w$. 

As mentioned earlier, CCA results in vectors $\bs\psi_{\X}$, $\bs\psi_{\Y}$ that are not sparse, and these vectors are not unique if $p > n$. In certain biomedical imaging applications if $p$ is large, one might be interested in finding a linear combination of the variables in $\X_w$ and $\Y_w$ that has large correlation but is also sparse in the variables used. To this end, several sparse versions of CCA have been proposed. ~\cite{parkhomenko} use penalized matrix decomposition to enforce sparsity;~\cite{parkhomenko} use soft-max thresholding in an iterative algorithm and~\cite{lykou} propose a LARS~\cite{lars} style algorithm for getting sparsity in loadings.

However, none of the previous works provide a way of ``partialing'' out the confounding variables-- a highly desirable modeling property for many biomedical and neuroimaging applications. 

In the next section, we describe the main contribution of this paper i.e. a way of ``partialing'' out these confounding variables in the SCCA setting and also provide a power iteration based algorithm to efficiently compute the directions of maximum correlation. Our work is a natural extension of the SCCA formulations by~\cite{parkhomenko,witten} and is called Partial Sparse CCA (PSCCA).



\section{PSCCA (Partial Sparse Canonical Correlation Analysis)}
As earlier, let $\X$ be the matrix with columns containing voxels from one set of
images of $n$ subjects; $\Y$ is the matrix with columns containing voxels from the second
set of images from the same $n$ subjects and further let $\Z$ be the matrix of confounding variables (age, diagnosis, gender etc.) for our neuroimaging problem. Also, let $\lambda_{\X}$ and $\lambda_{\Y}$ ($\in [0,1]$) (higher indicates more sparsity) be the user defined sparsity parameters which control the sparsity for either set of the canonical variates. They can be chosen automatically (``data driven'') so as to maximize the correlation (or likelihood) between the canonical variates, but such an approach would not naturally render itself  to scientific interpretability which is very important for neuroimaging studies. 

The PCCA~\cite{timm} finds the correlation between $\X$ and $\Y$ after removing (``partialing out'') the linear effect of the confounding variables $\Z$. 
We denote the $\X$ and $\Y$ matrices with effect of $\Z$ ``partialed'' out as $\X^{\backslash\Z}$ and $\Y^{\backslash\Z}$. Regressing $\X$ against $\Z$, using standard least squares ($\|\X -\Z\bs\beta\|^2$) gives $\bs\beta=  \bs\Sigma_{\Z\Z}^{-1}\Z^T\X$. So, the residual~\footnote{Note that $\X^{\backslash\Z}$ is actually what is called as the residual $\X-\Z\bs\beta$ in a least squares regression problem.} can be written as  $\X^{\backslash\Z}=\X - \Z\bs\Sigma_{\Z\Z}^{-1}\Z^T\X$. Applying the whitening transform to $\Z$  as $\Z_w =\Z \Sigma_{\Z\Z}^{-1/2}$, we get $\X^{\backslash\Z}=\X - \Z_w\Z_w^T\X$.
We can write similar equations for the residual when $\Y$ is regressed against $\Z$.
 
Now, we can write the complete variance-covariance matrix of the residuals as below:

\begin{eqnarray}
\label{matrices}
\begin{bmatrix}
  \bs\Sigma_{\X\X}^{\backslash \Z} & \bs\Sigma_{\X\Y}^{\backslash \Z} \\
  \bs\Sigma_{\Y\X}^{\backslash \Z} & \bs\Sigma_{\Y\Y}^{\backslash \Z} 
\end{bmatrix}
&=&
\begin{bmatrix}
  \X^T\X -\X^T \Z_w\Z_w^T\X &\;\;\;\;   \X^T\Y -\X^T \Z_w\Z_w^T\Y\\
  \Y^T\X -\Y^T \Z_w\Z_w^T\X &\;\;\;\;   \Y^T\Y -\Y^T \Z_w\Z_w^T\Y
\end{bmatrix}
\end{eqnarray}


Equation~(\ref{matrices}) can easily be extended to handle more than two matrices and also to ``partial'' out the effect of more than one confounding covariate matrices.


So, the PCCA problem can be written as:

\begin{equation}
\label{pscca}
\rho_{PCCA}=\max_{{\bs\phi_{\Y}}, {\bs\phi_{\X}}} \frac{\bs\phi_{\X}^T \bs\Sigma_{\X\Y}^{\backslash \Z}\bs\phi_Y}{\sqrt{\bs\phi_{\X}^T\bs\Sigma_{\X\X}^{\backslash \Z}\bs\phi_{\X}}\sqrt{\bs\phi_{\Y}^T\bs\Sigma_{\Y\Y}^{\backslash \Z}\bs\phi_{\Y}}}
\end{equation}


Now, changing the basis as for simple CCA, we get

\begin{equation}
\label{basisChangePSCCA}
\bs\psi_{\X} = \bs(\Sigma_{\X\X}^{\backslash \Z})^{1/2}\bs\phi_{\X}, \;\;\;\;\;\;   \bs\psi_{\Y} = \bs(\Sigma_{\Y\Y}^{\backslash \Z})^{1/2}\bs\phi_{\Y} 
\end{equation}

Now, substituting~(\ref{basisChangePSCCA}) in~(\ref{pscca}) we get 

\begin{equation}
\label{subsPSCCA}
\rho_{PCCA}= \max_{{\bs\psi_{\Y}}, {\bs\psi_{\X}}} \frac{\bs\psi_{\X}^T \bs(\Sigma_{\X\X}^{\backslash \Z})^{-1/2}\bs\Sigma_{\X\Y}\bs(\Sigma_{\Y\Y}^{\backslash \Z})^{-1/2}\bs\psi_{\Y}}{\|\bs\psi_{\X}\| \|\bs\psi_{\Y}\|}
\end{equation}

After some algebraic manipulation we can write the PCCA objective compactly as

\begin{equation}
\label{psccacompact}
\rho_{PCCA}=\max_{{\bs\psi_{\Y}}, {\bs\psi_{\X}}} \frac{\bs\psi_{\X}^T \bs\Sigma_{\X_w\Y_w}^{\backslash \Z}\bs\psi_{\Y}}{\|\bs\psi_{\X}\| \|\bs\psi_{\Y}\|}
\end{equation}
where $\X_w=\X\bs(\Sigma_{\X\X}^{\backslash \Z})^{-1/2}$ and $\Y_w=\Y\bs(\Sigma_{\Y\Y}^{\backslash \Z})^{-1/2}$. Note the difference in whitening transform from the one used in simple CCA; here we are using the covariance matrix with $\Z$ ``partialed'' out, to whiten.


Finally, the above objective after incorporating the user specified $\ell_1$ sparsity penalties ($\lambda_{\X}$ and $\lambda_{\Y}$) and under the constraints $\bs\psi_{\X}^T\bs\psi_{\X}=\bs\psi_{\Y}^T\bs\psi_{\Y}=1$ can be finally written as:

\begin{equation}
\label{psccaSparseConst}
\rho_{PSCCA}=\max_{{\bs\psi_{\Y}}, {\bs\psi_{\X}}} \{ \bs\psi_{\X}^T \bs\Sigma_{\X_w\Y_w}^{\backslash \Z}\bs\psi_{\Y} - \lambda_{\X}\|\psi_{\X}\|_1 - \lambda_{\Y}\|\psi_{\Y}\|_1\}
\end{equation}
One could use LARS~\cite{lars} style methods to enforce sparsity, however we use soft thresholding due to its simplicity and solve  (\ref{psccaSparseConst}) in an iterative fashion as we describe in next subsection.



%\begin{figure}
%\begin{center}
%\begin{tabular}{ccc}
% \includegraphics[width=70mm]{r16slice.png} &
% \includegraphics[width=70mm]{r16icm.png} \\
%(a) & (b) & (c) \\
%\end{tabular}
%\end{center}


%\caption{\baselineskip 12pt \small Cartoons illustrating (a) SCCA ;
%  (b) partial SCCA ; (c) part SCCA.  }
%\label{fig:cartoon}
%\end{figure}

\subsection{PSCCA Algorithm}
Using (\ref{matrices}), we propose a power iteration based algorithm for PSCCA which is similar to the ones proposed by~\cite{golub} for the general problem of finding principal eigenvectors of the matrices.


\begin{algorithm}[htdp]
\small \caption{\bf Computing principal eigenvectors for PSCCA}
\label{partial-mic}
\begin{algorithmic}[1]
\STATE Apply the whitening transformation to {\Z} to get $\Z_{w}$.
\STATE Compute $\X^{\backslash \Z}$ and $\Y^{\backslash \Z}$ and the whitened matrices $\X_{w}$ and $\Y_{w}$. 
\STATE Select the (fractional) sparsity parameters $\lambda_{\X}$ and $\lambda_{\Y}$
\STATE Randomly initialize $\bs \psi_{\X}^0$ and $\bs \psi_{\Y}^0$ ($\sim \mathcal{N}(0,1)$) and set $k=0$.

\WHILE {$\Delta$ Corr($\bs X_w \bs \psi_{\X}^{k+1}$, $\bs Y_w \bs \psi_{\Y}^{k+1}$) $<$ $\epsilon$}
\STATE Compute  $\bs \psi_{\X}^{k+1}= {\X_w}^T {\Y_w} \bs \psi_{\Y}^{k} -  {\X_w}^T  {\Z_w} {\Z_w}^T {\Y_w} \bs \psi_{\Y}^{k}$
\STATE Soft-Max Sparseness:  $\bs \psi_{\X}^{k+1} \leftarrow (\|\bs \psi_{\X}^{k+1}\|  - max(\bs \psi_{\X}^{k+1})*\lambda_{\X})_+ Sign(\bs \psi_{\X}^{k+1})$
\STATE Normalize: $\bs \psi_{\X}^{k+1} \leftarrow \frac{\bs \psi_{\X}^{k+1}}{\|\bs \psi_{\X}^{k+1}\|}$\\
//Repeat Same Procedure for $\bs \psi_{\Y}$ \\
\STATE Compute  $\bs \psi_{\Y}^{k+1}= {\Y_w}^T {\X_w} \bs \psi_{\X}^{k+1} -  {\Y_w}^T  {\Z_w} {\Z_w}^T {\X_w} \bs \psi_{\X}^{k+1}$
\STATE Soft-Max Sparseness: $\bs \psi_{\Y}^{k+1} \leftarrow (\|\bs \psi_{\Y}^{k+1}\|  - max(\bs \psi_{\Y}^{k+1})*\lambda_{\Y})_+ Sign(\bs \psi_{\Y}^{k+1})$
\STATE Normalize: $\bs \psi_{\Y}^{k+1} \leftarrow \frac{\bs \psi_{\Y}^{k+1}}{\|\bs \psi_{\Y}^{k+1}\|}$
\STATE k $\leftarrow$ k+1
\ENDWHILE
\end{algorithmic}
\end{algorithm}
where $(x)_+$ in the above Algorithm is equal to $x$ is $x \geq 0$ and $0$ is $x <0$


%\noindent{}
%\vspace{-0.1in}
%\begin{description}
%\item [Whiten:]Apply the whitening transformation to {\X}, {\Y}, {\Z}.
%    VectorType temp=q*w_q;
%    wpnew=p.transpose()*( temp - this->m_MatrixRRt*temp ); 
%\item [Begin Loop:]for power iteration. 
%\item [~~View 1:]Compute  $\x= {\X}^T {\Y} \y -  {\X}^T  {\Z} {\Z}^T {\Y} \y$.
%\item [~~Soft-Max Sparseness \& Normalization $\x$:] Enforce $\x$
%  sparseness and set $\x \leftarrow \frac{\x}{\|\x\|}$.
%\item [~~View 2:]Compute  $\y= {\Y}^T {\X} \x -  {\Y}^T  {\Z} {\Z}^T
%  {\X} \x$
%\item [~~Soft-Max Sparseness \& Normalization $\y$:] As in $\x$ step.
%\item [~~CC:]Compute $Corr( {\X} \x ,  {\Y} \y )$.
%\item [End loop:]Check the correlation and stop when converged.
%\end{description}
 
%For multiple eigenvectors, use the Lanczos algorithm.
Paramveer: Done till here
\section{Results}
\subsection{synthetic data}
we simulate three view data with and without a significant association
between the primary and secondary variables.  this analysis can be
achieved by simulating imaging data and two other views (age,
cognition) in such a way that the age is the true hidden variable that
generates both cognition and imaging measurements.  pscca should then
detect an insignificant association between cognition and imaging when
age is used as the confounding variable.  similarly, pscca should
detect a significant association between age and imaging when
cognition is a confounding variable.   

\subsection{real data versus regression}
Apply to one of our imaging datasets, potentially Phil's OCT data.
Alternatively, the oasis data. 

We employ a subset of the freely available OASIS dataset and compare
regression and pscca-based detection of gray matter effects
attributable to dementia.

PSCCA shows significant effects using a global multivariate test at
the $p<0.05$ level, permutation tested.  

Regression does not reveal signficant effects at the FDR-corrected
$q<0.05$ level.  

Selected slices of the results compare the regression-based p-value
map to the pscca weight vector map.  

we use only the primary eigenvector from pscca.  future work will
analyze the effect of including additional eigenvectors. 

\section{Discussion}
%\vspace{-0.1in}

\noindent{\bf Acknowledgment}
 This work is supported by Grant XXX 
% 1R01EB006266-01 
% from the ...
%National Institute Of Biomedical Imaging and Bioengineering and administered through the UCLA Center for Computational Biology.
% %\vspace{-0.1in}
\bibliographystyle{IEEEbib}
\bibliography{./cca}

\end{document}
