\documentclass{llncs}
\newcommand{\Section}[1]{\vspace{-8pt}\section{\hskip-1em.~~#1}\vspace{-3pt}}
\newcommand{\SubSection}[1]{\vspace{-3pt}\subsection{\hskip -1em.~~#1}\vspace{-3pt}}
\newcommand{\X}{{\bf X}}
\newcommand{\x}{{\bf x}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\y}{{\bf y}}
\newcommand{\Z}{{\bf Z}}
\newcommand{\z}{{\bf z}}
\usepackage{amsmath,amssymb}
\usepackage{times}
\usepackage{setspace,verbatim}
\usepackage{epsfig,url}

\begin{document}
\vspace{-0.1in}
\title{Partial Sparse Canonical Correlation Analysis (PSCCA) for population
  studies in Neuroimaging}
\author{Anonymous}
\institute{Anonymous}
\maketitle              
%\vspace{-0.1in}
\begin{abstract}
Sparse canonical correlation analysis (SCCA) is a powerful,
multivariate statistical tool for making unbiased inferences about the
relationship between different types of measurements taken on the same
population. Generally, SCCA depends upon univariate models to
factor out the effects of confounding variables.  In this paper we present a
novel algorithm for computing partial sparse canonical correlation
analysis (PSCCA) and factor (``partial'') out the effect of unwanted covariates. PSCCA employs
three views (modalities) of the data with one of them treated as a special set of
measurements whose effect on either  one or both other modalities should be factored out.  For instance, the relationship between
cognition and neuroanatomy should be measured independently of the
effect of age and gender.  We provide  theory and algorithms underlying partial sparse
canonical correlation analysis (PSCCA) via a representative neuroimaging
application.  
\end{abstract}
%\vspace{-0.2in}
\section{Introduction}
% pubmed references to MRI :  
% 2000-2001 --- 25561
% 2001-2002 --- 27053
% 2003-2004 --- 31708
% 2005-2006 --- 38620
% 2008-2009 --- 49288
% 2009-2010 --- 49323
% pubmed references to MRI brain :  
% 2000-2001 --- 9938
% 2001-2002 --- 
% 2003-2004 --- 12655
% 2005-2006 --- 
% 2008-2009 --- 
% 2009-2010 --- 19676
The number of neuroimaging studies published annually doubled from
9938 in 2000-2001 to 19676 in 2009-2010
(\url{http://www.ncbi.nlm.nih.gov/pubmed/}).  This growth has been
accompanied by increasing diversity in the types of data being
collected such that neuroimaging studies include not only various
structural and functional modalities but also neurocognitive
batteries, genetics and environmental measurements.  At the same
time, the statistical methods used in neuroimaging have changed
relatively little from twenty years ago with the exception of a small
number of recent studies \cite{Tosun2010a}.

%Why SCCA
%Why PSCCA

Consider a standard neuroimaging population study.  Each subject in
the study is between 20 and 40 years of age, has a quantitative
imaging measurement, is a member of one of two groups (patient or
control) and is either male or female.  A standard regression analysis
will treat the quantitative imaging measurement as the dependent
variable ($\y$) and age, gender and diagnosis group as covariates $({\X})$ where
each test is performed independently at each voxel.  Detection power
is compromised by the multiple-comparison problem incurred by the
number of imaging measurements (millions) as well as the confounding
variables (age, gender).  If we are using linear regression then we can not do much about the first problem but for the second problem we can factor out the effects of these unwanted (confounding) variables by regressing on the residuals ($\y -\X\beta$). 


An alternative methodology that one can use to approach the above neuroimaging  problem is by using CCA (Canonical Correlation Analysis)~\cite{hotellingcca} which inherently handles the multiple-comparison problem arising from large number of imaging measurements as it maximizes the correlation between the two views of the data ($\Y$ and $\X$ in our case)~\footnote{Note that here {$\X$} is same as earlier but $\Y$ is a matrix of all the imaging measurements and not a vector as was the case for linear regression study. As far as notation is concerned, we denote matrices by uppercase bold fonts and vectors by lowercase bold fonts, throughout this paper. }. Recently, sparse versions of CCA~\cite{parkhomenko,witten,lykou} have been proposed which give sparse loadings and work in the high dimensional setting, $p >n$, where $n$ is the number of observations (number of subjects in our neuroimaging study) and $p$ is the number of covariates. 

We are unaware of any previous work which studies factoring (``partialing'') out the confounding covariates in the case of Sparse CCA. In this paper, we provide the theory of Partial Sparse CCA (PSCCA) and present a novel and efficient iterative algorithm for PSCCA. As mentioned earlier, PSCCA like CCA does not do multiple-comparisons but rather performs a single global sparse multivariate test  over the whole region of interest (e.g. all voxels in the cortex). The sparseness parameter in
PSCCA selects the subset of the brain most associated with the
variable of interest (diagnosis) while factoring out confounding
effects (age, gender).  It is up to the researcher to use his or her
domain knowledge to select the desired sparseness for a given problem
or to experiment with a training dataset to optimize the sparseness
parameter.

The rest of the paper is organized as follows, in the next Section we provide a brief review of CCA and Sparse CCA. In Section 3, we present the theory behind Partial Sparse CCA (PSCCA) and describe our novel iterative algorithm for performing PSCCA. In Section 4, we provide experimental results on synthetic and real world neuroimaging datasets and we conclude with a brief summary in Section 5.

%What we are doing here ( a few sentences )

%This paper will detail and illustrate our approach to performing
%neuroimaging studies in the style of traditional formulations but
%using a new, powerful multivariate pscca.  We highlight in both
%simulated and real data the advantages and disadvantages of this
%method.

%\vspace{-0.2in}
\section{PSCCA Theory}
SCCA will maximize:
\begin{equation}
\text{argmax}( \x,\y) :
~\text{Corr}~( \X \x , \Y \y) - \lambda_\x \| \x \|_1 - \lambda_\y \|  \y  \|_1 , 
\end{equation} 
where $\X$ is a matrix with columns containing voxels from one set of
images of $n$ subjects, 
and $\Y$ is a matrix with columns containing voxels from the second
set of images from the same $n$ subjects. 
Corr computes Pearson correlation and the
$\lambda$ are inversely related to the sparseness costs, $C$.  %\vspace{-0.2in}

The covariance formulation of SCCA .... 

Let's compute the canonical correlation between two matrices where we
assume the matrices have been normalized and, as such, CCA computes
$$ \rho = \frac{ x \X^T \Y y  }{ \sqrt{x  \X^T \X x}\sqrt{x  \Y^T \Y y}  } $$.  Now, we change bases by
using the whitening transform.  
Redefine $\x =  ... $ Then, $\X \leftarrow \X \Sigma^{-1/2}_{XX}$ (same for $\Y$) and
$$ \rho = \frac{ x \X_w^T \Y_w \y  }{ ||x|| || y||} $$.
$$\rho = \frac{c^T \Sigma^{-1/2}_{XX} \Sigma_{XY}
  \Sigma^{-1/2}_{YY}}{\sqrt{c^Tc}\sqrt{d^Td}}$$

if matrices are whitened, then $\X \leftarrow \X \Sigma^{-1/2}_{XX}$
$$ \rho = \frac{c^T \Sigma_{XY}  d }{\sqrt{c^Tc}\sqrt{d^Td}}$$

The partial SCCA formulation will maximize 
\begin{equation}
\text{argmax}( \x,\y) :
~\text{Corr}~( \X_{\Z} \x , \Y_{\Z} \y) - \lambda_\x \| \x \|_1 - \lambda_\y \|  \y  \|_1 , 
\end{equation} 
where $\Z$ is a matrix of confounding variables and $\X_{\Z}$ and
$\Y_{\Z}$ represent $\X$, $\Y$ with the effect of $\Z$ factored out.

{\bf notes:}

One should use SCCA to test for relationships between {\X} and {\Z} and, separately, {\Y} and {\Z}.  Results will dictate whether
one should use standard, part or partial SCCA to assess the {\X} and {\Y}
relationship.

PSCCA will not apply sparseness to the covariate view.  

Follow Timm's notation.  Let's use {\X}, {\Y} and {\Z} to describe the theory, with {\Z} as the
'covariate'.   However, our code uses P, Q and R variables ....
 

\begin{figure}
\begin{center}
\begin{tabular}{ccc}
% \includegraphics[width=70mm]{r16slice.png} &
% \includegraphics[width=70mm]{r16icm.png} \\
(a) & (b) & (c) \\
\end{tabular}
\end{center}
\caption{\baselineskip 12pt \small Cartoons illustrating (a) SCCA ;
  (b) partial SCCA ; (c) part SCCA.  }
\label{fig:cartoon}
\end{figure}

\section{PSCCA Algorithm}
\vspace{0.1in}
\noindent{\bf Algorithm for computing first set of canonical variates}
\vspace{-0.1in}
\begin{description}
\item [Whiten:]Apply the whitening transformation to {\X}, {\Y}, {\Z}.
%    VectorType temp=q*w_q;
%    wpnew=p.transpose()*( temp - this->m_MatrixRRt*temp ); 
\item [Begin Loop:]for power iteration. 
\item [~~View 1:]Compute  $\x= {\X}^T {\Y} \y -  {\X}^T  {\Z} {\Z}^T {\Y} \y$.
\item [~~Soft-Max Sparseness \& Normalization $\x$:] Enforce $\x$
  sparseness and set $\x \leftarrow \frac{\x}{\|\x\|}$.
\item [~~View 2:]Compute  $\y= {\Y}^T {\X} \x -  {\Y}^T  {\Z} {\Z}^T
  {\X} \x$
\item [~~Soft-Max Sparseness \& Normalization $\y$:] As in $\x$ step.
\item [~~CC:]Compute $Corr( {\X} \x ,  {\Y} \y )$.
\item [End loop:]Check the correlation and stop when converged.
\end{description}
 
For multiple eigenvectors, use the Lanczos algorithm.

\section{Results}
\subsection{synthetic data}
we simulate three view data with and without a significant association
between the primary and secondary variables.  this analysis can be
achieved by simulating imaging data and two other views (age,
cognition) in such a way that the age is the true hidden variable that
generates both cognition and imaging measurements.  pscca should then
detect an insignificant association between cognition and imaging when
age is used as the confounding variable.  similarly, pscca should
detect a significant association between age and imaging when
cognition is a confounding variable.   

\subsection{real data versus regression}
Apply to one of our imaging datasets, potentially Phil's OCT data.
Alternatively, the oasis data. 

We employ a subset of the freely available OASIS dataset and compare
regression and pscca-based detection of gray matter effects
attributable to dementia.

PSCCA shows significant effects using a global multivariate test at
the $p<0.05$ level, permutation tested.  

Regression does not reveal signficant effects at the FDR-corrected
$q<0.05$ level.  

Selected slices of the results compare the regression-based p-value
map to the pscca weight vector map.  

we use only the primary eigenvector from pscca.  future work will
analyze the effect of including additional eigenvectors. 

\section{Discussion}
%\vspace{-0.1in}

\noindent{\bf Acknowledgment}
 This work is supported by Grant XXX 
% 1R01EB006266-01 
% from the ...
%National Institute Of Biomedical Imaging and Bioengineering and administered through the UCLA Center for Computational Biology.
% %\vspace{-0.1in}
\bibliographystyle{IEEEbib}
\bibliography{./cca}

\end{document}
